<style type="text/css">
    *.elegant {
        margin-left: 20px;
        margin-right: 120px;
        letter-spacing: 0.1px;
        word-spacing: 0.1px;
        line-height: 1.2em;
        text-indent: 0px;
        text-align: justify;
    }
</style> 
<p class="elegant">In this project page, we mainly introduce CDbin. </p>

<br />
<br />

<p class="elegant"><font color="blue" size=7><strong>Description to CDbin</strong></font></p>

<font color="red" size=3><strong>Abstract</strong></font> </font>
<p class="elegant">
<img src="./CSVTyjmFig1.pdf" width="1100"><br /><br /><br />
As an important computer vision task, image matching requires efficient and discriminative local descriptors. Most of existing descriptors like SIFT and ORB are hand-crafted. It is necessary to study more optimized descriptors through end-to-end learning. This paper proposes compact binary descriptors learned with a lightweight Convolutional Neural Network (CNN), which is efficient for training and testing. Specifically, we propose a CNN with no larger than five layers for descriptor learning. The resulting descriptors, i.e., Compact Discriminative binary descriptors (CDbin) are optimized with four complementary loss functions, i.e.,<br /><br />
1) triplet loss to ensure the discriminative power,<br /> 
2) quantization loss to decrease the quantization error, <br />
3) correlation loss to ensure the feature compactness,<br />
4) even-distribution loss to enrich the embedded information. <br />
Extensive experiments on two image patch datasets and three image retrieval datasets show that CDbin exhibits competitive performance compared with existing descriptors. For example, 64-bit CDbin substantially outperforms 256-bit ORB and 1024-bit SIFT on Hpatches dataset. Although generated by a shallow CNN, CDbin also outperforms several recent deep descriptors.
</p>


<font color="red" size=3><strong>Performance comparison of different architectures</strong></font> </font>
<p class="elegant">
<img src="./CSVTyjmFig2.pdf" width="1100"><br /><br /><br />
Fig. 2. mAP of CDbin descriptors generated by different networks in retrieval tasks defined by HPatches. Network architecture
CDbin(x-k) represents a k-bit CDbin descriptor extracted by a x-layer convolutional network. The network forward time, i.e.,
average time for a descriptor extraction is also compared.
</p>

<font color="red" size=3><strong>Performance comparison of different architectures</strong></font> </font>
<p class="elegant">
<img src="./CSVTyjmFig3.pdf" width="1100"><br /><br /><br />
Fig. 3. mAP of 256-bit CDbin on the retrieval task of HPatches dataset by setting different loss weights.
</p>

<font color="red" size=3><strong>Performance comparison of different architectures</strong></font> </font>
<p class="elegant">
<img src="./CSVTyjmFig5.pdf" width="1100"><br /><br /><br />
Fig. 5. mAP of CDbin(4-256) learned with different settings of training batch size on retrieval tasks defined by Hpatches. The solid and dashed lines represent real-valued and binary descriptors, respectively.
</p>

<font color="red" size=3><strong>Performance comparison of different architectures</strong></font> </font>
<p class="elegant">
<img src="./CSVTyjmFig6.pdf" width="1100"><br /><br /><br />
Fig. 6. Examples of patch retrieval results on HPatches dataset. The results of SIFT, ORB, and CDbin(5-256) are compared. In each example, the top 5 most similar patches to the query are presented, where the true positive and false positive are annotated with green and red dots, respectively.
</p>

<font color="red" size=3><strong>Performance comparison of different architectures</strong></font> </font>
<p class="elegant">
<img src="./CSVTyjmFig7.pdf" width="1100"><br /><br /><br />
Fig. 7. Examples of patch verification results on HPatches dataset. Each example shows the top 4 false matched pairs and their positions in ranking list. Larger ranking positions means the descriptor is more discriminative in identifying false matches.
</p>

<font color="red" size=3><strong>Performance comparison of different architectures</strong></font> </font>
<p class="elegant">
<img src="./CSVTyjmFig8.pdf" width="1100"><br /><br /><br />
Fig. 8. Sample image matching results on HPatches dataset. CDbin(5-256) and SIFT use MSER as keypoint detector. ORB use Harris detector.
</p>

<font color="red" size=3><strong>Performance comparison of different architectures</strong></font> </font>
<p class="elegant">
<img src="./CSVTyjmFig9.pdf" width="1100"><br /><br /><br />
Fig. 9. ROC curves of CDbin(5-256) and other binary descriptors on Brown. We use ND, YOS, LIB to denote Notredame, Yosemite, and Liberty respectively. Best viewed in color.
</p>

<font color="red" size=3><strong>Performance comparison of different architectures</strong></font> </font>
<p class="elegant">
<img src="./CSVTyjmTab4.pdf" width="1100"><br /><br /><br />
Comparison of mAP with other descriptors on three tasks defined by Hpatches. † denotes using deeper CNNs than ours. SP denotes supervised methods, USP denotes unsupervised methods and HC denotes hand-crafted methods.
</p>

<font color="red" size=3><strong>Performance comparison of different architectures</strong></font> </font>
<p class="elegant">
<img src="./CSVTyjmTab6.pdf" width="1100"><br /><br /><br />
Comparison of fpr95 with other descriptors on Brown dataset. In each column, dataset name with underline denotes the training set, the other one denotes the testing set. And test of Liberty(LIB), Notredame(ND), Yosemite(YOS) denotes supervised methods, all the combinations of the trian are shown. † denotes using deeper CNNs than ours. SP denotes supervised methods, USP denotes unsupervised methods and HC denotes hand-crafted methods.
</p>

<!-- <p class="elegant">
<table CellSpacing=1 WIDTH=80% border=1 cellpadding="0" >
    <tr>
    <td>Dataset</td> <td>MSMT17</td> <td>Duke [1][8] </td> <td>Market [2] </td> <td>CUHK03 [3] </td> <td>CUHK01 [4] </td> <td>VIPeR [5] </td> <td>PRID [6] </td> <td>CAVIAR [7] </td>
    </tr>
    <tr>
    <td>BBoxes</td> <td>126,441</td> <td>36,411</td> <td>32,668</td> <td>28,192</td> <td>3,884</td> <td>1,264</td> <td>1,134</td> <td>610</td>
    </tr>
    <tr>
    <td>Identities</td> <td>4,101</td> <td>1,812</td> <td>1,501</td> <td>1,467</td> <td>971</td> <td>632</td> <td>934</td> <td>72</td>
    </tr>
    <tr>
    <td>Cameras</td> <td>15</td> <td>8</td> <td>6</td> <td>2</td> <td>10</td> <td>2</td> <td>2</td> <td>2</td>
    </tr>
    <tr>
    <td>Detector</td> <td>Faster RCNN</td> <td>hand</td> <td>DPM</td> <td>DPM,hand</td> <td>hand</td> <td>hand</td> <td>hand</td> <td>hand</td>
    </tr>
    <tr>
    <td>Scene</td> <td>outdoor,indoor</td> <td>outdoor</td> <td>outdoor</td> <td>indoor</td> <td>indoor</td> <td>outdoor</td> <td>outdoor</td> <td>indoor</td>
    </tr>
</table>
</p> -->


<p class="elegant">
If  you use this dataset in your research, please kindly cite our work as,<br />
<textarea rows="7" cols="115" readonly="true">
<!-- @inproceedings{wei2018cvpr,
  title={Person Trasfer GAN to Bridge Domain Gap for Person Re-Identification},
  author={Wei, Longhui and Zhang, Shiliang and Gao, Wen and Tian, Qi},
  booktitle={Computer Vision and Pattern Recognition, IEEE Conference on},
  year={2018}
} -->
</textarea>
</p>




<font color="red" size=3><strong>Reference</strong></font> </font>
<p class="elegant">
<!-- [1] Z. Zheng et al. Unlabeled samples generated by gan improve the person re-identification baseline in vitro. In ICCV, 2017.<br/>
[2] L. Zheng et al. Scalable person re-identification: A benchmark. In ICCV, 2015.<br/>
[3] W. Li et al. Deepreid: Deep filter 918 pairing neural network for person re-identification. In CVPR, 2014.<br/>
[4] W. Li et al. Human reidentification with transferred metric learning. In ACCV, 2012.<br/>
[5] D. Gray et al. Viewpoint invariant pedestrian recogni- tion with an ensemble of localized features. In ECCV, 2008.<br/>
[6] M. Hirzer et al. Person re-identification by descriptive and discriminative classifica- tion. In SCIA, 2011.<br/>
[7] D. S. Cheng et al. Custom pictorial structures for re-identification. In BMVC, 2011.<br/>
[8] E. Ristani et al. Performance measures and a data set for multi-target, multi-camera tracking. In ECCV Workshop, 2016.<br/> -->
</p>